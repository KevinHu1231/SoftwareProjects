{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1QdDOSxXY9KqiBz0HT_mHwjtTX8Nacpvg","timestamp":1679531316903},{"file_id":"1ic0hZyLOyaEQTu_4k3a9RXOCW82Rz8m7","timestamp":1678383436408},{"file_id":"1AgSsbx6leKolyGWbLsa7-D8O77uzz50z","timestamp":1678381325150},{"file_id":"1h50RYTcRbJltS3DvVNzituNbO-yxP1aY","timestamp":1678297116019},{"file_id":"1Nr58vOSGwo2pxT96Y4Cmjfu8seTgkK0x","timestamp":1675736433023},{"file_id":"1KOtiVs8GRppl1Fp_uMDS6jQ-BXcw03UP","timestamp":1675181785989},{"file_id":"13zHEPU3AQNveSnDAZ2VJqsSKKzO8Liml","timestamp":1644209147522}],"collapsed_sections":["-bf1iLXTkXd5"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["Changelog:  (Last Updated 2023-03-09)"],"metadata":{"id":"gMAXaY8USfWb"}},{"cell_type":"markdown","source":["# Probabilistic ML: Assignment 3\n","- **Deadline**: 2023-03-27 (March 27th 2023)\n","- **Submission**: You need to submit your solutions through MarkUs, including all your derivations, plots, and your code. You can produce the files however you like (e.g. $\\LaTeX$, Microsoft Word, etc), as long as it is readable. Points will be deducted if we have a hard time reading your solutions or understanding the structure of your code.\n","- **Collaboration policy**: After attempting the problems on an individual basis, you may discuss and work together on the assignment with up to two classmates. However, **you must write your own code and write up your own solutions individually and explicitly name any collaborators** at the top of the homework."],"metadata":{"id":"248_qtulv8jx"}},{"cell_type":"markdown","metadata":{"id":"t1RKToQQcX2Q"},"source":["# 1. [54pts] Stochastic Variational Inference in the TrueSkill Model\n","\n","## Background\n","\n","We'll continue working with [TrueSkill](http://papers.nips.cc/paper/3079-trueskilltm-a-bayesian-skill-rating-system.pdf) model, a player ranking system for competitive games originally developed for Halo 2. Recall the model:\n","\n","\n","## Model definition\n","\n","We assume that each player has a true, but unknown skill $z_i \\in \\mathbb{R}$.\n","We use $N$ to denote the number of players.\n","\n","### The prior:\n","The prior over each player's skill is a standard normal distribution, and all player's skills are *a priori* independent.\n","\n","### The likelihood:\n","For each observed game, the probability that player $i$ beats player $j$, given the player's skills $z_A$ and $z_B$, is:\n","$$p(A \\,\\, \\text{beat} \\,\\, B | z_A, z_B) = \\sigma(z_i - z_j)$$\n","where\n","$$\\sigma(y) = \\frac{1}{1 + \\exp(-y)}$$\n","We chose this function simply because it's close to zero or one when the player's skills are very different, and equals one-half when the player skills are the same.  This likelihood function is the only thing that gives meaning to the latent skill variables $z_1 \\dots z_N$.\n","\n","There can be more than one game played between a pair of players. The outcome of each game is independent given the players' skills.\n","We use $M$ to denote the number of games.\n"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"n6euHm_h4Voi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679753944980,"user_tz":240,"elapsed":23750,"user":{"displayName":"Kevin Hu","userId":"17709586114698521274"}},"outputId":"e817e9d4-63f9-479c-be2f-7b703d0c575a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting wget\n","  Downloading wget-3.2.zip (10 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: wget\n","  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9676 sha256=e9179d3e14b09e4283887f8b9cc0beb2fa2848d44c5b248bf2400f64cf2d6689\n","  Stored in directory: /root/.cache/pip/wheels/04/5f/3e/46cc37c5d698415694d83f607f833f83f0149e49b3af9d0f38\n","Successfully built wget\n","Installing collected packages: wget\n","Successfully installed wget-3.2\n"]}],"source":["!pip install wget\n","import os\n","import os.path\n","\n","import matplotlib.pyplot as plt\n","import wget\n","\n","import pandas as pd\n","\n","\n","import numpy as np\n","from scipy.stats import norm\n","import scipy.io\n","import scipy.stats\n","import torch \n","import random\n","from torch import nn\n","from torch.distributions.normal import Normal\n","\n","from functools import partial\n","from tqdm import trange, tqdm_notebook\n","\n","import matplotlib.pyplot as plt\n","\n","# Helper function\n","def diag_gaussian_log_density(x, mu, std):\n","    # axis=-1 means sum over the last dimension.\n","    m = Normal(mu, std)\n","    return torch.sum(m.log_prob(x), axis=-1)"]},{"cell_type":"markdown","metadata":{"id":"-bf1iLXTkXd5"},"source":["## Implementing the TrueSkill Model\n","\n"]},{"cell_type":"markdown","metadata":{"id":"A62Y2IWg39ix"},"source":["This part was mostly done in Assignment 2. We will recall some useful functions.\n","\n","**a)** The function $\\texttt{log_joint_prior}$ computes the log of the prior, jointly evaluated over all player's skills."]},{"cell_type":"code","execution_count":2,"metadata":{"id":"PDmHDiac4Rpr","executionInfo":{"status":"ok","timestamp":1679753944981,"user_tz":240,"elapsed":12,"user":{"displayName":"Kevin Hu","userId":"17709586114698521274"}}},"outputs":[],"source":["def log_joint_prior(zs_array):\n","    return diag_gaussian_log_density(zs_array, torch.tensor([0.0]), torch.tensor([1.0]))"]},{"cell_type":"markdown","metadata":{"id":"AAbAgxMl4X2q"},"source":["**b)** The function `logp_a_beats_b` that, given a pair of skills $z_a$ and $z_b$, evaluates the log-likelihood that player with skill $z_a$ beat player with skill $z_b$ under the model detailed above.\n","\n","To ensure numerical stability, we use the function `np.log1p` that computes $\\log(1 + x)$ in a numerically stable way.  Or even better, use `np.logaddexp`."]},{"cell_type":"code","execution_count":3,"metadata":{"id":"V7nUXkYl_f4T","executionInfo":{"status":"ok","timestamp":1679753944982,"user_tz":240,"elapsed":12,"user":{"displayName":"Kevin Hu","userId":"17709586114698521274"}}},"outputs":[],"source":["def logp_a_beats_b(z_a, z_b):\n","    return -torch.logaddexp(torch.tensor([0.0]), z_b - z_a)\n","\n","def log_prior_over_2_players(z1, z2):\n","    m = Normal(torch.tensor([0.0]), torch.tensor([[1.0]]))\n","    return m.log_prob(z1) + m.log_prob(z2)\n","\n","def prior_over_2_players(z1, z2):\n","    return torch.exp(log_prior_over_2_players(z1, z2))\n","\n","def log_posterior_A_beat_B(z1, z2):\n","    return log_prior_over_2_players(z1, z2) + logp_a_beats_b(z1, z2)\n","\n","def posterior_A_beat_B(z1, z2):\n","    return torch.exp(log_posterior_A_beat_B(z1, z2))\n","\n","def log_posterior_A_beat_B_10_times(z1, z2):\n","    return log_prior_over_2_players(z1, z2) + 10.0 * logp_a_beats_b(z1, z2)\n","\n","def posterior_A_beat_B_10_times(z1, z2):\n","    return torch.exp(log_posterior_A_beat_B_10_times(z1, z2))\n","\n","def log_posterior_beat_each_other_10_times(z1, z2):\n","    return log_prior_over_2_players(z1, z2) \\\n","        + 10.* logp_a_beats_b(z1, z2) \\\n","        + 10.* logp_a_beats_b(z2, z1)\n","\n","def posterior_beat_each_other_10_times(z1, z2):\n","    return torch.exp(log_posterior_beat_each_other_10_times(z1, z2))"]},{"cell_type":"markdown","source":["The following functions will be used for plotting.  Note that `plot_2d_fun` can now take an optional second function, so you can compare two functions."],"metadata":{"id":"hA77ZVR5SgGs"}},{"cell_type":"code","execution_count":4,"metadata":{"id":"at7JEyWs_QRK","executionInfo":{"status":"ok","timestamp":1679753944982,"user_tz":240,"elapsed":12,"user":{"displayName":"Kevin Hu","userId":"17709586114698521274"}}},"outputs":[],"source":["# Plotting helper functions for free\n","def plot_isocontours(ax, func, xlimits=[-4, 4], ylimits=[-4, 4], steps=101, cmap=\"summer\"):\n","    x = torch.linspace(*xlimits, steps=steps)\n","    y = torch.linspace(*ylimits, steps=steps)\n","    X, Y = torch.meshgrid(x, y)\n","    Z = func(X, Y)\n","    plt.contour(X, Y, Z, cmap=cmap)\n","    ax.set_yticks([])\n","    ax.set_xticks([])\n","\n","def plot_2d_fun(f, x_axis_label=\"\", y_axis_label=\"\", f2=None, scatter_pts=None):\n","    # This is the function your code should call.\n","    # f() should take two arguments.\n","    fig = plt.figure(figsize=(8,8), facecolor='white')\n","    ax = fig.add_subplot(111, frameon=False)\n","    ax.set_xlabel(x_axis_label)\n","    ax.set_ylabel(y_axis_label)\n","    plot_isocontours(ax, f)\n","    if f2 is not None:\n","      plot_isocontours(ax, f2, cmap='winter')\n","    \n","    if scatter_pts is not None:\n","      plt.scatter(scatter_pts[:,0], scatter_pts[:, 1])\n","    plt.plot([4, -4], [4, -4], 'b--')   # Line of equal skill\n","    plt.show(block=True)\n","    plt.draw()"]},{"cell_type":"markdown","metadata":{"id":"nm-SM6Fc4yz8"},"source":["## **1.1 [20pts]** Stochastic Variational Inference on Two Players and Toy Data\n","\n","One nice thing about a Bayesian approach is that it separates the model specification from the approximate inference strategy.\n","The original Trueskill paper from 2007 used message passing.\n","\n","In this question we will  approximate posterior distributions with gradient-based stochastic variational inference.\n","\n","The parameters are $\\phi = (\\mu,\\log(\\sigma))$. Notice that instead of $\\sigma$ (which is constrained to be positive), we work with $\\log(\\sigma)$, removing the constraint. This way, we can do unconstrained gradient-based optimization.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"K-ri8HRQ461m"},"source":["**a) [9pts]** Implement the missing lines in the below code, to complete the evidence lower bound function and the reparameterized sampler for the approximate posterior.\n","\n","Hint 1: You must use the reparametrization trick in your sampler if you want your gradients to be unbiased.\n","\n","Hint 2: If you're worried you got these wrong, you can check that the sampler matches the log pdf by plotting a histogram of samples against a plot of the pdf."]},{"cell_type":"code","execution_count":6,"metadata":{"id":"kZlCOViq5Ahf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679753956116,"user_tz":240,"elapsed":141,"user":{"displayName":"Kevin Hu","userId":"17709586114698521274"}},"outputId":"30d93905-9329-4e6f-8e35-f9052b908796"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[   0.9549,  -23.6124],\n","        [   1.0630,  -45.9519],\n","        [   0.9728,    3.9402],\n","        [   1.0280,   89.9260],\n","        [   1.0487,   41.2272],\n","        [   1.0024,   48.6096],\n","        [   1.0332,  -28.7329],\n","        [   1.0529,   52.2333],\n","        [   1.0544, -123.8428],\n","        [   0.9847,  -36.0202]])\n","tensor([-3.3611, -4.2191, -2.8615, -4.1698, -3.6101, -3.0041, -3.1273, -3.9028,\n","        -6.1836, -2.9507])\n"]}],"source":["from numpy.core.fromnumeric import mean\n","from scipy.special import stdtr\n","def diag_gaussian_samples(mean, log_std, num_samples):\n","    # mean and log_std are (D) dimensional vectors\n","    # Return a (num_samples, D) matrix, where each sample is\n","    # from a diagonal multivariate Gaussian.\n","\n","    # TODO.  You might want to use torch.randn(). Remember\n","    # you must use the reparameterization trick.  Also remember that\n","    # we are parameterizing the _log_ of the standard deviation.\n","\n","    #Calculate q_phi\n","    D = np.shape(mean)[0]\n","\n","    samples = torch.zeros(num_samples,D)\n","    for i in range(num_samples):\n","        epsilon = torch.randn(D)\n","        std = torch.exp(log_std)\n","        z = mean + torch.mul(epsilon,std)\n","        samples[i,:] = z\n","    \n","    return samples\n","\n","def diag_gaussian_logpdf(x, mean, log_std):\n","    # Evaluate the density of a batch of points on a \n","    # diagonal multivariate Gaussian. x is a (num_samples, D) matrix.\n","    # Return a tensor of shape (num_samples)\n","    \n","    return diag_gaussian_log_density(x,mean,torch.exp(log_std))\n","\n","def batch_elbo(logprob, mean, log_std, num_samples):\n","    # TODO: Use simple Monte Carlo to estimate ELBO\n","    # on a batch of size num_samples\n","    \n","    return \n","\n","mean = torch.tensor([1,2])\n","log_std = torch.tensor([-3.2,4])\n","s = diag_gaussian_samples(mean,log_std,10)\n","print(s)\n","pdf = diag_gaussian_logpdf(s, mean, log_std)\n","print(pdf)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"lfAD3ffb5CD9"},"source":["\n","**b) [4pts]** Write a loss function called $\\texttt{objective}$  that takes variational distribution parameters, and returns an unbiased estimate of the negative elbo using $\\texttt{num_samples_per_iter}$ samples, to approximate the joint posterior over skills conditioned on observing player A winning 10 games.\n","\n","Note: We want a _negative_ ELBO estimate, because the convention in optimization is to minimize functions, and we want to maximize the ELBO."]},{"cell_type":"code","source":["# Hyperparameters\n","num_players = 2\n","n_iters = 800\n","stepsize = 0.0001\n","num_samples_per_iter = 50\n","\n","def log_posterior_A_beat_B_10_times_1_arg(z1z2):\n","  return log_posterior_A_beat_B_10_times(z1z2[:,0], z1z2[:,1]).flatten()\n","\n","def objective(params):  # The loss function to be minimized.\n","  # TODO.  Hint:  This can be done in one line.\n","  return "],"metadata":{"id":"j-Qt2vqvKgAN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**c) [1pts]** Initialize a set of variational parameters and optimize them to approximate the joint where we observe player A winning 10 games. Report the final loss. Also plot the optimized variational approximation contours and the target distribution on the same axes.\n","\n","Hint:  Almost initialization should be fine.  How many variational parameters do you need?"],"metadata":{"id":"ixGDUDjwKnWd"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"biW5JiZq5EXc"},"outputs":[],"source":["def callback(params, t):\n","  if t % 25 == 0:\n","    print(\"Iteration {} lower bound {}\".format(t, objective(params)))\n","\n","# Set up optimizer.\n","D = 2\n","# init_log_std  = # TODO.\n","# init_mean = # TODO\n","\n","params = (init_mean, init_log_std)\n","optimizer = torch.optim.SGD(params, lr=stepsize, momentum=0.9)\n","\n","def update():\n","    optimizer.zero_grad()\n","    loss = objective(params)\n","    loss.backward()\n","    optimizer.step()\n","\n","# Main loop.\n","print(\"Optimizing variational parameters...\")\n","for t in trange(0, n_iters):\n","    update()\n","    callback(params, t)\n","\n","\n","def approx_posterior_2d(z1, z2):\n","    # The approximate posterior\n","    mean, logstd = params[0].detach(), params[1].detach()\n","    return torch.exp(diag_gaussian_logpdf(torch.stack([z1, z2], dim=2), mean, logstd))\n","\n","plot_2d_fun(posterior_A_beat_B_10_times, \"Player A Skill\", \"Player B Skill\",\n","            f2=approx_posterior_2d)"]},{"cell_type":"markdown","metadata":{"id":"sCmqGbeK5iAf"},"source":["**d) [3pts]** Write a loss function called $\\texttt{objective}$  that takes variational distribution parameters , and returns a negative elbo estimate using simple Monte carlo with $\\texttt{num_samples_per_iter}$ samples, to approximate the joint where we observe player A winning 10 games and player B winning 10 games.\n","\n","Hint:  You can find analogous functions in the code above.\n"]},{"cell_type":"code","source":["# Hyperparameters\n","n_iters = 100\n","stepsize = 0.0001\n","num_samples_per_iter = 50\n","\n","def log_posterior_beat_each_other_10_times_1_arg(z1z2):\n","    # z1z2 is a tensor with shape (num_samples x 2)\n","    # Return a tensor with shape (num_samples)\n","\n","    return\n","\n","def objective(params):\n","    return"],"metadata":{"id":"5Zz6DkOacXDC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**e) [3pts]** Run the code below to optimize, and report the final loss. Also plot the optimized variational approximation contours and the target distribution on the same axes.\n","\n","---\n","\n","Write one or two sentences describing the joint settings of skills that are plausible under the true posterior, but which are not plausible under the approximate posterior.\n","\n","--- \n","\n","Finally, answer with one or two sentences:  Would changing the variational approximate posterior from a fully-factorized (diagonal covariance) Gaussian to a non-factorized (fully parameterized covariance) Gaussian make a better approximation in this instance?"],"metadata":{"id":"_V42MMIscvMs"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"ksjggALU5gWx"},"outputs":[],"source":["# Main loop.\n","print(\"Optimizing variational parameters...\")\n","for t in trange(0, n_iters):\n","    update()\n","    callback(params, t)\n","\n","plot_2d_fun(posterior_beat_each_other_10_times, \"Player A Skill\", \"Player B Skill\",\n","            f2=approx_posterior_2d)"]},{"cell_type":"markdown","metadata":{"id":"rrCmtd_b51Jf"},"source":["## 1.2 [34 pts] Approximate inference conditioned on real data \n","\n","The dataset contains data on 2546 chess games amongst 1434 players:\n"," - names is a 1434 by 1 matrix, whose $i$’th entry is the name of player $i$.\n"," - games is a 2546 by 2 matrix of game outcomes (actually chess matches), one row per game.\n","\n","The first column contains the indices of the players who won.\n","The second column contains the indices of the player who lost.\n","\n","It is based on the kaggle chess dataset: https://www.kaggle.com/datasets/datasnaek/chess\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9Dbf_GOZ7ejb"},"outputs":[],"source":["wget.download(\"https://michalmalyska.github.io/csc412/chess_games.csv\")\n","games = pd.read_csv(\"chess_games.csv\")[[\"winner_index\", \"loser_index\"]].to_numpy()\n","wget.download(\"https://michalmalyska.github.io/csc412/chess_players.csv\")\n","names = pd.read_csv(\"chess_players.csv\")[[\"index\", \"player_name\"]].to_numpy()\n","\n","games = torch.LongTensor(games)"]},{"cell_type":"markdown","metadata":{"id":"dkYB8hS07fOW"},"source":["\n","\n","**a) [0pt]** Assuming all game outcomes are i.i.d. conditioned on all players' skills, the function $\\texttt{log_games_likelihood}$ takes a batch of player skills $\\texttt{zs}$ and a collection of observed games $\\texttt{games}$ and gives the total log-likelihood for all those observations given all the skills.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pkW93RBf7pru"},"outputs":[],"source":["def log_games_likelihood(zs, games):\n","    winning_player_ixs = games[:,0]\n","    losing_player_ixs = games[:,1]\n","\n","    winning_player_skills = zs[:, winning_player_ixs] \n","    losing_player_skills = zs[:, losing_player_ixs]\n","\n","    log_likelihoods = logp_a_beats_b(winning_player_skills, losing_player_skills)\n","    return torch.sum(log_likelihoods, dim=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uaPEulQoAndL"},"outputs":[],"source":["def log_joint_probability(zs):\n","    return log_joint_prior(zs) + log_games_likelihood(zs, games)"]},{"cell_type":"markdown","metadata":{"id":"aUWILTA18BRn"},"source":["**b) [4pt]** Write a new objective function like the one from the previous question. \n","\n","Below, we initialize a variational distribution and fit it to the joint distribution with all the observed tennis games from the dataset."]},{"cell_type":"code","source":["# Hyperparameters\n","num_players = 1434\n","n_iters = 500\n","stepsize = 0.0001\n","num_samples_per_iter = 150\n","\n","def objective(params):\n","    return"],"metadata":{"id":"qsCQUJCFd2Eb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**c) [3pts]** Optimize, and report the final loss. "],"metadata":{"id":"O3ep43C5d_CT"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"ock35XuW8EK8"},"outputs":[],"source":["# Set up optimizer.\n","init_mean = torch.zeros(num_players, requires_grad=True)\n","init_log_std  = torch.zeros(num_players, requires_grad=True)\n","params = (init_mean, init_log_std)\n","optimizer = torch.optim.SGD(params, lr=stepsize, momentum=0.9)\n","\n","def update():\n","    optimizer.zero_grad()\n","    loss = objective(params)\n","    loss.backward()\n","    optimizer.step()\n","\n","# Optimize and print loss in a loop\n","# HINT: you can use the callback() function to report loss\n"]},{"cell_type":"markdown","metadata":{"id":"y6YluZ2B8Qmr"},"source":["**d) [1pt]** Plot the approximate mean and variance of all players, sorted by skill."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sEZtYBeH8TOO"},"outputs":[],"source":["# mean_skills, logstd_skills = # TODO.  Hint: You don't need to do simple Monte Carlo here.\n","# Hint: You should use .detach() before you do anything with the params tensors\n","mean_skills, logstd_skills = \n","order = torch.argsort(mean_skills)\n","\n","plt.xlabel(\"Player Rank\")\n","plt.ylabel(\"Player Skill\")\n","plt.errorbar(range(num_players), )"]},{"cell_type":"markdown","metadata":{"id":"EjWom7_Z8VCY"},"source":["**e) [1pts]** List the names of the 10 players with the highest mean skill under the variational model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4LCPM5Bp8YiK"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"sEj6LkYA8ncH"},"source":["**f) [3pt]** Plot samples from the joint posterior over the skills of lelik3310 and thebestofthebad. Based on your samples, describe in a sentence the relationship between the skills of the players. (Is one better than the other? Are they approximately even?)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U6zI2sCB8qFO"},"outputs":[],"source":["lelik3310_ix = 496\n","thebestofthebad_ix = 512\n","print(names[lelik3310_ix])\n","print(names[thebestofthebad_ix])\n","\n","fig = plt.figure(figsize=(8,8), facecolor='white')\n","\n","# Label each with \"<player> Skill\"\n","plt.xlabel(\"lelik3310 Skill\") \n","plt.ylabel(\"thebestofthebad Skill\") \n","\n","plt.plot([3, -3], [3, -3], 'b--') # Line of equal skill\n","\n","samples = diag_gaussian_samples(mean_skills, logstd_skills, 100)\n","\n","# TODO:  Hint:  Use plt.scatter()"]},{"cell_type":"markdown","metadata":{"id":"rUXcsAYdDnor"},"source":["**g) [8pts]** Derive the exact probability under a factorized Gaussian over two players’ skills that one has higher skill than the other, as a function of the two means and variances over their skills. Express your answer in terms of the cumulative distribution function of a one-dimensional Gaussian random variable.\n","\n","- Hint 1: Use a linear change of variables $y_A, y_B = z_A − z_B , z_B$. What does the line of equal skill look like after this transformation?\n","- Hint 2: If $X \\sim \\mathcal N (\\mu, \\Sigma)$, then $AX \\sim \\mathcal N (A\\mu, A\\Sigma A^T)$ where $A$ is a linear transformation.\n","- Hint 3: Marginalization in Gaussians is easy: if $X \\sim \\mathcal N (\\mu, \\Sigma)$, then the $i$th element of $X$ has a\n","marginal distribution $X_i \\sim \\mathcal N (\\mu_i , \\Sigma_{ii})$."]},{"cell_type":"markdown","metadata":{"id":"OxazFdqcFhF_"},"source":["Your answer here."]},{"cell_type":"markdown","metadata":{"id":"GO3k7rfI8sWg"},"source":["**h) [4pts]** Compute the probability under your approximate posterior that lelik3310 has higher skill than thebestofthebad. Compute this quantity exactly using the formula you just derived above, and also estimate it using simple Monte Carlo with 10000 examples.\n","\n","Hint:  You might want to use `Normal(0,1).cdf()` for the exact formula."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oqXgDi-T-W7o"},"outputs":[],"source":["# TODO\n","def prob_A_superior_B(N, A_ix, B_ix):\n","    \n","\n","    return formula_est, mc_est\n","\n","formula_est, mc_est = prob_A_superior_B(10000, lelik3310_ix, thebestofthebad_ix)\n","print(f\"Exact CDF Estimate: {formula_est}\")\n","print(f\"Simple MC Estimate: {mc_est}\")"]},{"cell_type":"markdown","metadata":{"id":"olpPTnm3-YdN"},"source":["**i) [2pts]** Compute the probability that lelik3310 is better than the player with the 5th lowest mean skill. Compute this quantity exactly, and then estimate it using simple Monte Carlo with 10000 examples.\n"]},{"cell_type":"code","source":["# TODO\n","fifth_worst_ix = \n"],"metadata":{"id":"PNLfQCDdbZ7d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**j) [4 pts]** Imagine that we knew ahead of time that we were examining the skills of top chess players, and so changed our prior on all players to Normal(10, 1) and re-ran our approximate inference from scratch. Would that change the answer of either of the previous 2 questions, in expectation?"],"metadata":{"id":"kjslZo7WUH2-"}},{"cell_type":"markdown","source":["Your answer here."],"metadata":{"id":"W37pO-PDNjKc"}},{"cell_type":"markdown","source":["**k) [4 pts]** Based on all the plots and results in this assignment and HW2, which approximate inference method do you suspect is producing a better overall approximation to the true posterior over all skills conditioned on all games?  Give a short explanation."],"metadata":{"id":"cNJ5WuyQKPPM"}},{"cell_type":"markdown","source":["Your answer here."],"metadata":{"id":"mtmXbA3OKSoM"}},{"cell_type":"markdown","source":["# 2. [21pts] Question 2: VAE Warm up with synthetic data\n","\n","In this question, we will train a VAE on a synthetic data which resembles spirals in 2d. This question is intended to provided you some debugging tools for the next question where you work on MNIST dataset. In both of the questions, we will perform amortized inference with VAEs."],"metadata":{"id":"nDUMG4VJU4x5"}},{"cell_type":"markdown","source":["Function below generates the synthetic spiral data."],"metadata":{"id":"S-NI6vCde9rg"}},{"cell_type":"code","source":["# Code to generate the pinwheel dataset.\n","# Taken from [Johnson et al (2016)], updated by Zhao & Linderman.\n","def make_pinwheel_data(radial_std, tangential_std, num_classes, num_per_class, rate):\n","    rads = torch.linspace(0, 2*torch.pi, num_classes + 1)\n","\n","    features = torch.randn(num_classes*num_per_class, 2) * torch.tensor([radial_std, tangential_std])\n","    print(features)\n","    features[:, 0] = features[:, 0] + 1.0\n","    labels = torch.repeat_interleave(torch.arange(num_classes), num_per_class)\n","\n","    angles = rads[labels] + rate * torch.exp(features[:,0])\n","    rotations = torch.stack([np.cos(angles), -np.sin(angles), np.sin(angles), np.cos(angles)])\n","    rotations = torch.reshape(rotations.T, (-1, 2, 2))\n","\n","    perm_ix = torch.randperm(labels.shape[0])\n","    return labels[perm_ix], torch.einsum('ti,tij->tj', features, rotations)[perm_ix]"],"metadata":{"id":"uyiFI2Aie9UH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We visualize the 2d data generated from the above function. Notice that there are 3 clusters in the input space, each colored with a different color. The VAE will not see the cluster assignments, but we hope to recover this structure in the latent space."],"metadata":{"id":"_4HxOH1GWBno"}},{"cell_type":"code","source":["num_clusters = 3           \n","samples_per_cluster = 300\n","labels, data = make_pinwheel_data(0.3, 0.1, num_clusters, samples_per_cluster, 0.25)\n","\n","for k in range(num_clusters):\n","    plt.scatter(data[labels == k, 0], data[labels == k, 1], s=2)\n","\n","plt.axis(\"equal\")"],"metadata":{"id":"DUpKL-57dRR9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 2.1 [12 pts] Implement the missing lines in the below code, to complete the $\\texttt{elbo}$ function for a variational autoencoder.\n","\n","You can and should use the same code for both the spiral dataset and MNIST.  The spiral dataset and an example encoder / decoder is provided just to help you debug, and as a template for the MNIST VAE.\n"],"metadata":{"id":"QXlBa-8oFn9V"}},{"cell_type":"code","source":["# Generic VAE functions.\n","\n","def log_prior(zs_array):\n","    return diag_gaussian_log_density(zs_array, torch.tensor([0.0]), torch.tensor([1.0]))\n","\n","def diag_gaussian_samples(mean, log_std, num_samples):\n","    return mean + torch.exp(log_std) * torch.randn(num_samples, mean.shape[-1])\n","\n","def diag_gaussian_logpdf(x, mean, log_std):\n","    return diag_gaussian_log_density(x, mean, torch.exp(log_std))\n","\n","\n","def batch_elbo(  # Simple Monte Carlo estimate of the variational lower bound.\n","    recognition_net,    # takes a batch of datapoints, outputs mean and log_std of size (batch_size x latent_dim), i.e. log q(z|x)\n","    decoder_net,        # takes a batch of latent samples, outputs mean and log_std of size (batch_size x data_dim), i.e. log p(x|z)\n","    log_joint,          # takes decoder_net, a batch of latent samples, and a batch of datapoints, outputs unnormalized log joint, i.e. log p(x,z)\n","    data                # a.k.a. x\n","    ):\n","    # TODO.  Get posterior parameters\n","    # TODO.  Sample z from approximate posterior.\n","    # TODO.  Calculate the joint\n","    # TODO.  Calculate the log posterior.\n","    # TODO.  Produce an unbiased esimate of the ELBO.\n","    return #TODO: return monte carlo estimate"],"metadata":{"id":"ChZZaRmqhkvu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The below code trains a VAE where the encoder and decoder are both neural networks. The parameters are specified in the starter code.  You don't need to do anything here, this is just to help you debug."],"metadata":{"id":"lOyXE8CCHNwT"}},{"cell_type":"code","source":["from torch.utils.data import TensorDataset, DataLoader\n","# Now define a specific VAE for the spiral dataset\n","\n","data_dimension = 2\n","latent_dimension = 2\n","\n","# Define the recognition network.\n","class RecognitionNet(nn.Module):\n","    def __init__(self, data_dimension, latent_dimension):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(data_dimension, 100),\n","            nn.ReLU(),\n","            nn.Linear(100, 50),\n","            nn.ReLU()\n","        )\n","        self.mean_net = nn.Linear(50, latent_dimension) # Output mean of q(z)\n","        self.std_net = nn.Linear(50, latent_dimension) # Output log_std of q(z)\n","        \n","    def forward(self, x):\n","        interm = self.net(x)\n","        mean, log_std = self.mean_net(interm), self.std_net(interm)\n","        return mean, log_std\n","\n","recognition_net = RecognitionNet(data_dimension, latent_dimension)\n","\n","# Define the decoder network.\n","# Note that it has two outputs, a mean and a variance, because \n","# this model has a Gaussian likelihood p(x|z).\n","class Decoder(nn.Module):\n","    def __init__(self, latent_dimension, data_dimension):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(latent_dimension, 100),\n","            nn.ReLU(),\n","            nn.Linear(100, 50),\n","            nn.ReLU()\n","        )\n","        self.mean_net = nn.Linear(50, data_dimension) # Output mean of p(x|z)\n","        self.std_net = nn.Linear(50, data_dimension) # Output log_std of p(x|z)\n","        \n","    def forward(self, x):\n","        interm = self.net(x)\n","        mean, log_std = self.mean_net(interm), self.std_net(interm)\n","        return mean, log_std\n","\n","decoder_net = Decoder(latent_dimension, data_dimension)\n","\n","# Set up log likelihood function.\n","def log_likelihood(decoder_net, latent, data):\n","    mean, log_std = decoder_net(latent)\n","    return diag_gaussian_logpdf(data, mean,\n","                                np.log(0.1) + 0. * log_std)  # Note: we are cheating here and using a fixed noise variance to make optimization more stable.\n","\n","def log_joint(decoder_net, latent, data):\n","    return log_prior(latent) + log_likelihood(decoder_net, latent, data)\n","\n","# Run optimization\n","optimizer = torch.optim.Adam([{'params': recognition_net.parameters()},\n","                      {'params': decoder_net.parameters()}], lr=1e-3)\n","n_iters = 2000\n","minibatch_size = 300\n","\n","dataset = TensorDataset(torch.tensor(data))\n","dataloader = DataLoader(dataset, batch_size=minibatch_size, shuffle=True)\n","\n","def objective(recognition_net, decoder_net):  # The loss function to be minimized.\n","    minibatch = next(iter(dataloader))[0]\n","    return -batch_elbo(\n","    recognition_net,\n","    decoder_net,\n","    log_joint,\n","    minibatch)\n","\n","def callback(t):\n","    if t % 100 == 0:\n","        print(\"Iteration {} lower bound {}\".format(t, -objective(recognition_net, decoder_net)))\n","\n","def update():\n","    optimizer.zero_grad()\n","    loss = objective(recognition_net, decoder_net)\n","    loss.backward()\n","    optimizer.step()\n","\n","# Main loop.\n","print(\"Optimizing variational parameters...\")\n","for t in trange(0, n_iters):\n","    update()\n","    callback(t)"],"metadata":{"id":"OgGgHfUsmid6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 2.2 [5 pts]  In this part, we visualize how the data looks like in the latent space. We simply use the trained recognition network (the encoder) to map each input to latent space.\n"],"metadata":{"id":"ItI3GUaJH2o7"}},{"cell_type":"code","source":["# Show the means of the encoded data in a 2D latent space.\n","# Don't worry if this doesn't look much like a Gaussian.\n","\n","for k in range(num_clusters):\n","    # cur_data =  # TODO get all the data from this cluster.\n","    # transformed =  # TODO find the mean of q(z|x) for each x. Remember to .detach() any tensors\n","\n","    plt.scatter(transformed[:, 0], transformed[:, 1], s=2)\n","    print(transformed.shape)\n","\n","plt.axis(\"equal\")\n","plt.xlabel(\"latent dimension 1\")\n","plt.ylabel(\"latent dimension 2\")"],"metadata":{"id":"u5GjmVm636GK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 2.3 [4pts] Generate new data using the decoder and the generative model we just trained.\n","\n","For this, we simply generate 1000 latent variables in the latent space from the prior and pass it through the decoder network.\n","\n","You shouldn't expect this to match the data exactly, just to get the overall shape and number of clusters roughly correct.  \n"],"metadata":{"id":"FeSyYzKhINyK"}},{"cell_type":"code","source":["# Sample data from the trained generative model to see if it\n","# roughly matches the data.  # Note: This doesn't add the likelihood noise,\n","# although it should if we want it to match the data.\n","\n","num_samples = 1000\n","# samples =  # TODO\n","# transformed =  # TODO\n","\n","plt.scatter(transformed[:, 0], transformed[:, 1], s=2)\n","plt.axis(\"equal\")"],"metadata":{"id":"YYm4QeiGk_Jm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Here's a debugging tool only available when both the latent space and the data are both 2-dimensional.  We can show the function being learned by the encoder by showing how it warps a 2D grid into the latent space."],"metadata":{"id":"Z3ndLm3JggYd"}},{"cell_type":"code","source":["from matplotlib.collections import LineCollection\n","\n","def plot_grid(x,y, ax=None, **kwargs):\n","    ax = ax or plt.gca()\n","    segs1 = np.stack((x,y), axis=2)\n","    segs2 = segs1.transpose(1,0,2)\n","    ax.add_collection(LineCollection(segs1, **kwargs))\n","    ax.add_collection(LineCollection(segs2, **kwargs))\n","    ax.autoscale()\n","\n","def f(x,y):\n","    xy = torch.stack([x.flatten(), y.flatten()], dim=1)\n","    return decoder_net(xy)\n","\n","fig, ax = plt.subplots()\n","\n","grid_x,grid_y = torch.meshgrid(torch.linspace(-3,3,20),torch.linspace(-3,3,20))\n","plot_grid(grid_x,grid_y, ax=ax,  color=\"lightgrey\")\n","\n","distx, disty = f(grid_x,grid_y)\n","distx, disty = distx.reshape(20, 20, 2), disty.reshape(20, 20, 2)\n","plot_grid(distx[:, :, 0].detach().numpy(), distx[:, :, 1].detach().numpy(), ax=ax, color=\"C0\")\n","\n","plt.show()"],"metadata":{"id":"UgdwiGrpsHRP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Here, we show the function being learned by the decoder by showing how it warps a 2D grid into the observed space.\n","\n"],"metadata":{"id":"qX7XKlJLgqFz"}},{"cell_type":"code","source":["def f(x,y):\n","    xy = torch.stack([x.flatten(), y.flatten()], dim=1)\n","    return recognition_net(xy)\n","\n","fig, ax = plt.subplots()\n","\n","grid_x,grid_y = torch.meshgrid(torch.linspace(-3,3,20),torch.linspace(-3,3,20))\n","plot_grid(grid_x,grid_y, ax=ax,  color=\"lightgrey\")\n","\n","distx, disty = f(grid_x,grid_y)\n","distx, disty = distx.reshape(20, 20, 2), disty.reshape(20, 20, 2)\n","plot_grid(distx[:, :, 0].detach().numpy(), distx[:, :, 1].detach().numpy(), ax=ax, color=\"C0\")\n","\n","plt.show()"],"metadata":{"id":"DqkriclvSsaN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#3. [34pts] Question 3: VAE Real Data\n","\n","In this question, we will implement and investigate the Variational Autoencoder  as introduced by the paper [Auto-Encoding Variational Bayes](https://arxiv.org/pdf/1312.6114.pdf) by Kingma and Welling (2013) on binarized fashion MNIST data.\n","\n","The below code contains a function that downloads the fashion MNIST data set and another one that can plot multiple images.\n"],"metadata":{"id":"ItRbxaGrXNDI"}},{"cell_type":"code","source":["import numpy as np\n","import os\n","import gzip\n","import struct\n","import array\n","import matplotlib.pyplot as plt\n","import matplotlib.image\n","from urllib.request import urlretrieve\n","\n","def download(url, filename):\n","    if not os.path.exists('data'):\n","        os.makedirs('data')\n","    out_file = os.path.join('data', filename)\n","    if not os.path.isfile(out_file):\n","        urlretrieve(url, out_file)\n","\n","\n","def fashion_mnist():\n","    base_url = 'http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/'\n","\n","    def parse_labels(filename):\n","        with gzip.open(filename, 'rb') as fh:\n","            magic, num_data = struct.unpack(\">II\", fh.read(8))\n","            return np.array(array.array(\"B\", fh.read()), dtype=np.uint8)\n","\n","    def parse_images(filename):\n","        with gzip.open(filename, 'rb') as fh:\n","            magic, num_data, rows, cols = struct.unpack(\">IIII\", fh.read(16))\n","            return np.array(array.array(\"B\", fh.read()), dtype=np.uint8).reshape(num_data, rows, cols)\n","\n","    for filename in ['train-images-idx3-ubyte.gz',\n","                     'train-labels-idx1-ubyte.gz',\n","                     't10k-images-idx3-ubyte.gz',\n","                     't10k-labels-idx1-ubyte.gz']:\n","        download(base_url + filename, filename)\n","\n","    train_images = parse_images('data/train-images-idx3-ubyte.gz')\n","    train_labels = parse_labels('data/train-labels-idx1-ubyte.gz')\n","    test_images = parse_images('data/t10k-images-idx3-ubyte.gz')\n","    test_labels = parse_labels('data/t10k-labels-idx1-ubyte.gz')\n","    \n","    # Remove the data point that cause log(0)\n","    remove = (14926, 20348, 36487, 45128, 50945, 51163, 55023)\n","    train_images = np.delete(train_images,remove, axis=0)\n","    train_labels = np.delete(train_labels, remove, axis=0)\n","    return train_images, train_labels, test_images[:1000], test_labels[:1000]\n","\n","\n","def load_fashion_mnist():\n","    partial_flatten = lambda x: np.reshape(x, (x.shape[0], np.prod(x.shape[1:])))\n","    one_hot = lambda x, k: np.array(x[:, None] == np.arange(k)[None, :], dtype=int)\n","    train_images, train_labels, test_images, test_labels =  fashion_mnist()\n","    train_images = (partial_flatten(train_images) / 255.0 > .5).astype(float)\n","    test_images = (partial_flatten(test_images) / 255.0 > .5).astype(float)\n","    train_images, test_images = np.float32(train_images), np.float32(test_images)\n","    train_labels = torch.tensor(one_hot(train_labels, 10))\n","    test_labels = torch.tensor(one_hot(test_labels, 10))\n","    N_data = train_images.shape[0]\n","\n","    return N_data, train_images, train_labels, test_images, test_labels\n","\n","N_data, train_images, train_labels, test_images, test_labels = load_fashion_mnist()\n","\n","def plot_images(images, ims_per_row=5, padding=5, dimensions=(28, 28),\n","                cmap=matplotlib.cm.binary, vmin=0., vmax=1.):\n","    \"\"\"Images should be a (N_images x pixels) matrix.\"\"\"\n","    fig = plt.figure(1)\n","    fig.clf()\n","    ax = fig.add_subplot(111)\n","\n","    N_images = images.shape[0]\n","    N_rows = np.int32(np.ceil(float(N_images) / ims_per_row))\n","    pad_value = vmin\n","    concat_images = np.full(((dimensions[0] + padding) * N_rows + padding,\n","                             (dimensions[1] + padding) * ims_per_row + padding), pad_value)\n","    for i in range(N_images):\n","        cur_image = np.reshape(images[i, :], dimensions)\n","        row_ix = i // ims_per_row\n","        col_ix = i % ims_per_row\n","        row_start = padding + (padding + dimensions[0]) * row_ix\n","        col_start = padding + (padding + dimensions[1]) * col_ix\n","        concat_images[row_start: row_start + dimensions[0],\n","                      col_start: col_start + dimensions[1]] = cur_image\n","        cax = ax.matshow(concat_images, cmap=cmap, vmin=vmin, vmax=vmax)\n","        plt.xticks(np.array([]))\n","        plt.yticks(np.array([]))\n","    \n","    plt.plot()\n","\n","plot_images(train_images[:10, :])"],"metadata":{"id":"_KvgdSxIXSP4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 3.1 [15 pts] Implement the missing lines in the below code, and report the final ELBO\n","\n","### The model is as follows:\n","\n","*   **Prior:** The prior over each items's latent representation is a multivariate standard normal distribution. For all questions, we'll set the dimension of the latent space to 2.\n","A larger latent dimension would provide a more powerful model, but for this assignment we'll use a two-dimensional latent space to make visualization and debugging easier.\n","\n","\n","*   **Likelihood:** Given the latent representation $z$ for an image, the distribution over all 784 pixels in the image is given by a product of independent Bernoullis, whose means are given by the output of a neural network $f_\\theta(z)$:\n","$$p(x|z, \\theta) = \\prod_{d=1}^{784} \\operatorname{Ber}(x_d|f_\\theta(z)_d)$$\n","The neural network $f_\\theta$ is called the decoder, and its parameters $\\theta$ will be optimized to fit the data.\n","\n","### the functions to complete:\n","1.   $\\texttt{log_likelihood}$: Log-likelihood of the above model.\n","2.   $\\texttt{log_joint}$: Joint log-likelihood of data and latent variables.\n","3.   $\\texttt{objective}$: You will use the elbo function you wrote in the previous question.\n","\n","### Report the final ELBO. "],"metadata":{"id":"CuU3ftS4Fl-5"}},{"cell_type":"code","source":[" \n"," # Define a specific VAE for MNIST\n","\n","data_dimension = 28*28\n","latent_dimension = 2\n","\n","# Define the recognition network.\n","class RecognitionNet(nn.Module):\n","    def __init__(self, data_dimension, latent_dimension):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(data_dimension, 150),\n","            nn.ReLU(),\n","            nn.Linear(150, 50),\n","            nn.ReLU()\n","        )\n","        self.mean_net = nn.Linear(50, latent_dimension) # Output mean of q(z)\n","        self.std_net = nn.Linear(50, latent_dimension) # Output log_std of q(z)\n","        \n","    def forward(self, x):\n","        interm = self.net(x)\n","        mean, log_std = self.mean_net(interm), self.std_net(interm)\n","        return mean, log_std\n","\n","# recognition_net = RecognitionNet(YOUR-ANSWER-HERE, YOUR-ANSWER-HERE) # TODO: What are the input and output dimensions of the encoder?\n","\n","# Define the decoder network.\n","class Decoder(nn.Module):\n","    def __init__(self, latent_dimension, data_dimension):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(latent_dimension, 150),\n","            nn.ReLU(),\n","            nn.Linear(150, 50),\n","            nn.ReLU(),\n","            # nn.Linear(YOUR-ANSWER-HERE, YOUR-ANSWER-HERE) # TODO: Output logit of Ber(x|z)\n","        \n","    def forward(self, x):\n","        return self.net(x)\n","\n","# decoder_net = DecoderNet(YOUR-ANSWER-HERE, YOUR-ANSWER-HERE) # TODO: What are the input and output dimensions of the decoder?\n","\n","# Set up log likelihood function.\n","def bernoulli_logpdf(logits, x):\n","    \"\"\"Bernoulli log pdf of data x given logits.\"\"\"\n","    return -torch.sum(torch.logaddexp(torch.tensor([0.]), torch.where(x > 0.5, -1., 1.) * logits), dim=1)\n","\n","def log_likelihood(decoder_net, latent, data): # TODO\n","    return\n","\n","def log_joint(decoder_net, latent, data): # TODO\n","    return\n","\n","# Run optimization\n","optimizer = torch.optim.Adam([{'params': recognition_net.parameters()},\n","                      {'params': decoder_net.parameters()}], lr=1e-2)\n","n_iters = 5000\n","minibatch_size = 200\n","\n","dataset = TensorDataset(torch.tensor(train_images), )\n","dataloader = DataLoader(dataset, batch_size=minibatch_size, shuffle=True)\n","\n","def objective(recognition_net, decoder_net):  # The loss function to be minimized.  \n","    minibatch = # TODO\n","    return # TODO\n","\n","def callback(t):\n","    if t % 100 == 0:\n","        print(\"Iteration {} lower bound {}\".format(t, -objective(recognition_net, decoder_net)))\n","\n","def update():\n","    optimizer.zero_grad()\n","    loss = objective(recognition_net, decoder_net)\n","    loss.backward()\n","    optimizer.step()\n","\n","# Main loop.\n","print(\"Optimizing variational parameters...\")\n","for t in trange(0, n_iters):\n","    update()\n","    callback(t)\n","\n","callback(n_iters)"],"metadata":{"id":"enYQZKWebwFy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 3.2 [3pts]\n"," \n","In what follows, we will investigate our model by visualizing the distribution over data given by the generative model, sampling from it, and interpolating between items.\n","\n","One way to understand the meaning of latent representations is to see which parts of the latent space correspond to which kinds of data. Here we'll produce a scatter plot in the latent space, where each point in the plot represents a different image in the training set.\n","\n","Implement the missing lines in the below code, to plot the transformed data points in the latent space.\n","\n","\n","\n","1.   Encode each image in the training set.\n","2.   Take the 2D mean vector of each encoding $q_\\phi(z|x)$.\n","3.   Plot these mean vectors in the 2D latent space with a scatterplot.\n","4.   Colour each point according to the class label (0 to 9).\n","\n","\n","Hopefully our latent space will group images of different classes, even though we never provided class labels to the model!\n","\n","\n","\n"],"metadata":{"id":"_4D_ldpaGldW"}},{"cell_type":"code","source":["for k in range(10):\n","    cur_data = torch.tensor(train_images[train_labels[:, k] == True, :]).to(torch.float32)\n","    #   transformed = # TODO:  Call recognition net and extract mean, remember to .detach() the tensors!\n","    plt.scatter(transformed[:, 0], transformed[:, 1], s=2)\n","\n","plt.axis(\"equal\")"],"metadata":{"id":"MReco_Jje8LE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 3.3 [8 pts] \n","Now we'll fit a model with a larger latent space.  We won't be able to visualize this model as easily, but it should be able to fit the data better.  Set latent_dimension to 20 and re-fit the model.  You should be able to get a better ELBO using this larger model."],"metadata":{"id":"omqkiyxKRfRU"}},{"cell_type":"code","source":["# TODO: Train the same model as above but with latent_dimension = 20\n","# Feel free to copy code from above.\n"],"metadata":{"id":"bydpfpEjRnUL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 3.4) [4pts] Implement the missing lines in the below code, to plot the transformed data points in the latent space.\n","\n","You will be using ancestral sampling:\t\n","\n","1.   First sample a z from the prior.\n","2.   Use the generative model to compute the bernoulli means over the pixels of $x$ given $z$. \n","3.   Plot these means (parameters) as a greyscale image.\n","\n","Do this for 25 samples z from the prior."],"metadata":{"id":"Dk0GjGNOHEXc"}},{"cell_type":"code","source":["# Sample data from the trained generative model to see if it\n","# roughly matches the data.\n","\n","def sigmoid(x):\n","  # Maps logits to probabilities.\n","  return 0.5 * (torch.tanh(x) + 1.0)\n","\n","sampled_zs = # TODO: Sample from prior on z.\n","bernoulli_means = # TODO: Get logits, and map them to probabilities. Don't forget to .detach()\n","plot_images(bernoulli_means)"],"metadata":{"id":"mYkNjM1Ufq9s"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 3.5 [4pts]\n","\n","Another way to examine a latent variable model with continuous latent variables is to interpolate between the latent representations of two points. Here we will encode 3 pairs of data points with different classes. Then we will linearly interpolate between the mean vectors of their encodings. We will plot the generative distributions along the linear interpolation.\n","\n","\n","1.   First, write a function which takes two points $z_a$ and $z_b$, and a value $\\alpha \\in [0,1]$, and outputs the linear interpolation $z_\\alpha = \\alpha z_a + (1-\\alpha)z_b$.\n","2.  Sample 3 pairs of images, each having a different class.\n","3.  Encode the data in each pair, and take the mean vectors\n","4.  Linearly interpolate between these mean vectors\n","5.  At 10 equally-space points along the interpolation, plot the Bernoulli means $p(x|z_\\alpha)$.\n","6.  Concatenate these plots into one figure."],"metadata":{"id":"T_TLbol-Yrb2"}},{"cell_type":"code","source":["def plot_interp(ix1, ix2):\n","    left_z = recognition_net(torch.tensor(train_images[ix1, :]).to(torch.float32))[0].detach()\n","    right_z = recognition_net(torch.tensor(train_images[ix2, :]).to(torch.float32))[0].detach()\n","    interp_weights = list(torch.linspace(0.0, 1.0, 10))\n","    interp_zs =  # TODO: Linearly interpolate between left_z and right_z.  Feel free to use a for loop. You don't have to use interp_weights, but they might be helpful\n","    bernoulli_means = # TODO: \n","    plot_images(bernoulli_means)"],"metadata":{"id":"1Wt6FylTfygq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_interp(3, 11)"],"metadata":{"id":"VwbJ00RLQ1aB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_interp(11, 22)"],"metadata":{"id":"i7IrTnLbQ3oD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_interp(32, 334)"],"metadata":{"id":"m2R_U7-9Q46R"},"execution_count":null,"outputs":[]}]}